# Taller: Uso practico de Airflow

## ğŸ“Œ DescripciÃ³n

**Este repositorio contiene un entorno para el procesamiento de datos y entrenamiento de modelos basado en `Apache Airflow`, `Docker` y `Python`. Su diseÃ±o modular permite la automatizaciÃ³n de flujos de trabajo con soporte para `PostgreSQL`, `MySQL` y `Redis`, mejorando la gestiÃ³n y orquestaciÃ³n de tareas en entornos distribuidos.


El flujo de trabajo estÃ¡ diseÃ±ado para:

âœ… **Gestionar datos en MySQL** mediante los DAGS `data_loading` y `delete_data`.  
âœ… **Entrenar modelos automÃ¡ticamente con `Airflow`** usando `train.py`.  
âœ… **Hacer inferencias con `FastAPI`** sobre modelos previamente entrenados.  
âœ… **Orquestar servicios con `Docker`**, incluyendo bases de datos (`PostgreSQL`, `MySQL`) y `Redis`.  
âœ… **Facilitar la gestiÃ³n de tareas en entornos distribuidos** con `CeleryExecutor`.  

---

## ğŸ”„ **Flujo de Trabajo del Proyecto (DAGs en Airflow)**

Los flujos de trabajo estÃ¡n gestionados como **DAGs en Apache Airflow**, lo que permite ejecutar y programar cada proceso de manera automatizada.

1ï¸âƒ£ **`delete_data` (DAG en Airflow)**  
   ğŸ”¹ **Asegura que la base de datos MySQL estÃ© limpia antes de cargar nuevos datos.**  
   ğŸ”¹ **Elimina cualquier dato previo para evitar inconsistencias.**  

2ï¸âƒ£ **`data_loading` (DAG en Airflow)**  
   ğŸ”¹ **Lee el archivo `penguins_lter.csv` y carga los datos en MySQL.**  
   ğŸ”¹ **Deja la informaciÃ³n lista para su uso en entrenamiento.**  

3ï¸âƒ£ **`train.py` (DAG en Airflow)**  
   ğŸ”¹ **Entrena un modelo de Machine Learning con los datos cargados en MySQL.**  
   ğŸ”¹ **Guarda el modelo como `SVM_model.pkl` en `/opt/airflow/datos/`.**  

4ï¸âƒ£ **FastAPI (`main.py`)**  
   ğŸ”¹ **Carga el modelo entrenado desde `/opt/airflow/datos/SVM_model.pkl`.**  
   ğŸ”¹ **Expone el endpoint `/pinguino` para hacer inferencias.**  

ğŸ’¡ **Diagrama General del Flujo:**

```text
+-------------+       â‘  Elimina datos previos        +------------+
|  Airflow    |------------------------------------->| delete_data|
|  DAGs       |                                     | (DAG)      |
+-------------+                                     +------------+
      |
      |   â‘¡ Carga datos desde CSV en MySQL
      |--------------------------------------+
      |                                      â–¼
      |                            +------------------+
      |--------------------------->| data_loading (DAG)|
      |                            +------------------+
      |
      |   â‘¢ Entrena modelo con datos de MySQL
      |--------------------------------------+
      |                                      â–¼
      |                            +------------------+
      |--------------------------->| train.py (DAG)   |
      |                            | Guarda modelo    |
      |                            | SVM_model.pkl    |
      |                            +------------------+
      |
      |   â‘£ FastAPI carga modelo entrenado
      |--------------------------------------+
      |                                      â–¼
      |                            +------------------+
      |--------------------------->|  FastAPI (main.py)|
      |                            |  Predicciones    |
      |                            +------------------+
      |
      |    +----------------+
      |---->|   Usuario     |
           |  (Cliente API) |
           +----------------+
```
---

## ğŸ“‚ Estructura del Proyecto

La estructura del proyecto estÃ¡ organizada para garantizar una correcta separaciÃ³n de responsabilidades y modularidad. Los flujos de trabajo se encuentran en la carpeta dags/, donde cada script maneja tareas especÃ­ficas como la carga, eliminaciÃ³n y entrenamiento de datos. La carpeta app/ contiene los scripts principales de la aplicaciÃ³n, incluyendo el manejo de modelos y dependencias. La carpeta datos/ contiene los archivos de entrada requeridos para el procesamiento, mientras que logs/ almacena los registros de ejecuciÃ³n para facilitar el monitoreo. Los archivos de configuraciÃ³n, como docker-compose.yml y Dockerfile, permiten la implementaciÃ³n del entorno en contenedores, asegurando escalabilidad y reproducibilidad.

```
TALLER_3/
â”‚â”€â”€ app/                      # CÃ³digo de la aplicaciÃ³n principal
â”‚   â”œâ”€â”€ cargar_modelos.py     # Script para carga de modelos
â”‚   â”œâ”€â”€ Dockerfile.app        # Dockerfile especÃ­fico para la app
â”‚   â”œâ”€â”€ main.py               # Script principal
â”‚   â”œâ”€â”€ requirements.txt      # Dependencias especÃ­ficas de la app
â”‚â”€â”€ dags/                      # DefiniciÃ³n de flujos de trabajo
â”‚   â”œâ”€â”€ carga_datos.py         # Carga de datos
â”‚   â”œâ”€â”€ elimina_datos.py       # EliminaciÃ³n de datos
â”‚   â”œâ”€â”€ train.py               # Entrenamiento del modelo
â”‚â”€â”€ datos/                     # Almacenamiento de datos
â”‚   â”œâ”€â”€ penguins_lter.csv      # Conjunto de datos en formato CSV
â”‚   â”œâ”€â”€ SVM_model.pkl          # Modelos entrenados, se guardan luego de ejecutar train.py
â”‚â”€â”€ logs/                      # Registro de ejecuciÃ³n
â”‚â”€â”€ plugins/                   # Extensiones para Airflow
â”‚â”€â”€ docker-compose.yml         # OrquestaciÃ³n de servicios
â”‚â”€â”€ Dockerfile                 # ConstrucciÃ³n de la imagen Docker
â”‚â”€â”€ README.md                  # DocumentaciÃ³n del proyecto
â”‚â”€â”€ requirements.txt           # Dependencias generales del proyecto
```

---

## ğŸ›  Instancia MySQL
Para crear la instancia de la base de datos mysql, fue necesario incluir el servicio mysql dentro del docker-compose y el volume mysql_data como se encuentra a continuaciÃ³n:

```
mysql:
    # Nombre del servicio
    image: mysql:latest  # Imagen Docker para el contenedor MySQL
    ports:
      - "3306:3306"  # Mapeo de puertos del contenedor al host
    environment:
      MYSQL_ROOT_PASSWORD: airflow  # ContraseÃ±a de root para MySQL
      MYSQL_DATABASE: penguin_data  # Nombre de la base de datos MySQL
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]  # Comando de verificaciÃ³n de salud
      interval: 10s  # Frecuencia de la verificaciÃ³n de salud
      timeout: 5s  # Tiempo mÃ¡ximo de espera para la verificaciÃ³n de salud
      retries: 3  # NÃºmero de intentos de verificaciÃ³n de salud
      start_period: 10s  # Tiempo antes de iniciar las verificaciones de salud
    restart: always  # PolÃ­tica de reinicio del contenedor
```

## ğŸš€ **InstalaciÃ³n y ConfiguraciÃ³n**

### 1ï¸âƒ£ **Clonar el repositorio**

1. Clonar el repositorio
```
git clone https://github.com/JohnSanchez27/MLOps_Taller3.git
cd MLOps_Taller3

```
2. ConfiguraciÃ³n de permisosÃ§

Si hay errores al iniciar Airflow o FastAPI por problemas de escritura en directorios, ejecuta:

```
sudo chown -R $(whoami):$(whoami) datos logs
sudo chmod -R 777 datos logs

```

Si Airflow no tiene permisos para escribir en /opt/airflow/datos/, corrige dentro del contenedor:


```
docker exec -it airflow-worker bash
chmod -R 777 /opt/airflow/datos
exit
```

3. Construir el contenedor
```
docker-compose up --build
```

4. Acceder a las interfaces de Airflow y FastAPI

ğŸ“Œ Airflow UI
â¡ï¸ http://localhost:8080

ğŸ“Œ FastAPI Docs
â¡ï¸ http://localhost:8989/docs

5. Puedes verificar los contenedores activos con:

```
docker ps
```

Si alguno de los servicios no estÃ¡ corriendo, reinÃ­cialo con:

```
docker-compose restart <nombre_del_servicio>

```

ğŸš€ Â¡Listo! Ahora tienes todo configurado para entrenar modelos con Airflow y hacer inferencias con FastAPI. ğŸ‰ğŸ”¥

## ğŸ”¥ Retos del Uso de Airflow
El uso de Airflow en entornos productivos conlleva varios desafÃ­os tÃ©cnicos que deben ser considerados para su correcta implementaciÃ³n. La configuraciÃ³n inicial requiere definir correctamente las variables de entorno y la conexiÃ³n entre servicios. La gestiÃ³n de dependencias debe ser precisa para evitar incompatibilidades con la versiÃ³n de Airflow utilizada. El monitoreo y depuraciÃ³n de errores es esencial debido a la generaciÃ³n de mÃºltiples registros de ejecuciÃ³n. Para manejar grandes volÃºmenes de datos, es necesario optimizar la configuraciÃ³n de `CeleryExecutor` y los recursos asignados a `Redis`, `PostgreSQL` y `MySQL`. AdemÃ¡s, la estructuraciÃ³n adecuada de los DAGs permite optimizar la ejecuciÃ³n y reducir tiempos de procesamiento.

## ğŸ“Š Uso
El proyecto se gestiona a travÃ©s de la interfaz web de Airflow, donde se pueden visualizar, programar y ejecutar flujos de trabajo. Los DAGs ubicados en `dags/` pueden ser modificados para adaptarse a nuevos requerimientos o incorporar nuevas funcionalidades. Los registros de ejecuciÃ³n almacenados en `logs/` proporcionan informaciÃ³n detallada sobre el estado de cada tarea, lo que facilita la depuraciÃ³n y optimizaciÃ³n del sistema.

## ğŸŒ Vista de DAGs en Airflow

A continuaciÃ³n se muestra una captura de pantalla de la interfaz de Airflow, donde se visualizan los DAGs definidos para este proyecto:

![alt text](image-1.png)
